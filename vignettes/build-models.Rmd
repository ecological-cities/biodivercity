---
title: "Build Models"
subtitle: "Build and summarise GLMM models with processed animal and landscape data"
author: "Edwin Tan"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Build Models}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

Following the preparation of collected fauna and landscape data, this article outlines a workflow to build predictive models of animal diversity. Species density at sampling points were modelled against the summarised landscape metrics with generalised linear mixed-effects models (GLMMs) using the `MuMIn::dredge` function. However, owing to the large number of landscape predictors at multiple buffer radii generated from the remotely
sensed and open data, variable selection has to be performed as there is a limit to the number of variables `MuMIn::dredge` is allowed to accept.

## Combining fauna and landscape data

To build the models, animal and landscape datasets as processed in the previous two articles have to be combined and prepared.

First, load the required packages to run the analysis:

```{r load required libraries, message = FALSE, warning = FALSE, eval = FALSE}
library("biodivercity")
library("tidyverse") # to process/wrangle data
library("sf") # to process landscape data
library("tmap")
library("kableExtra")
```

```{r load biodivercity while in dev, include = FALSE}
devtools::load_all()
library("tidyverse") 
library("sf") 
library("tmap")
library("kableExtra")
```

In this short example, load the datasets and filter for `area == "PG"` and `period == 1` only.
```{r}
data(animal_observations)
data(animal_surveys)
data("sampling_points") 


observations <- animal_observations %>% 
  dplyr::filter(area == "PG") %>% 
  dplyr::filter(period == 1)

surveys <- animal_surveys %>% 
  dplyr::filter(area == "PG") %>% 
  dplyr::filter(period == 1)

points <- sampling_points %>% 
  dplyr::filter(area == "PG") %>% 
  dplyr::filter(period == 1)
```

Animal observations are then tallied to retrieve species richness at each point. The `tally_observations` function can be used with the argument `level = "point"`.

```{r}
birds <- tally_observations(observations,
                            surveys,
                            specify_taxon = "Aves",
                            specify_area = "PG",
                            specify_period = 1,
                            level = "point") %>%
  rename(sprich = n)
```

Re-calculate the landscape metrics for the area of Punggol with the `ndvi_mosaic.tif` raster by first classifying for vegetation with `classify_image_binary()`. 

```{r}
ndvi_mosaic <- 
  system.file("extdata", "ndvi_mosaic.tif", package="biodivercity") %>% 
  terra::rast()

veg_classified <- classify_image_binary(ndvi_mosaic, threshold = "otsu")
```

```{r, include = FALSE}
tmap_mode("view")
tmap_options(check.and.fix = TRUE)

tm_shape(ndvi_mosaic) +
  tm_raster(palette = c("Greens")) +
tm_shape(veg_classified) +
  tm_raster(title = "veg_classified",
            style = "cat",
            palette = c("grey", "darkgreen")) +
tm_shape(sampling_points) +
  tm_dots(col = "yellow")
```

The classified raster `veg_classified` is then input into `lsm_perpoint()` as per the `Process landscape` article to retrieve relevant landscape metrics.

```{r, eval = FALSE}
points <- lsm_perpoint(raster = veg_classified,
                                      points = points,
                                      buffer_sizes = c(100, 200), # in metres
                                      level = c("class"),
                                      class_names = c("veg"),
                                      class_values = c(1)) # pixel value of class
```

Finally, the two datasets will be combined with an `inner_join()`. An input dataset for model building will follow the same format.

```{r, eval = FALSE}
birds <- birds %>% 
  inner_join(points, 
             by = c("town", "round", "point_id"))

head(birds)
```

---

### If too many variables are present for `MuMIn::dredge`:

For this collection of articles, random forest models were first used to select variables based on their relative importance in improving model performance, i.e., averaged variable importance (Arthur et al., 2010; Li et al., 2017). The random forest algorithm is a machine learning technique that can handle and reduce the effects of overfitting, as well as the collinearity and selection bias of predictors (Hothorn et al., 2006). In the following sections, the example variables were selected using the `randomForest` package.

## Building the GLM models

Now, load the necessary packages to run the model building:

```{r load more libraries, message = FALSE, warning = FALSE}
library("lme4")
library("MuMIn")
```

Bird species density and landscape data for sampling points across six areas (towns) in Singapore from the example data `model_data` will be used to demonstrate the model building process in this article. The variable selection output using random forest is also loaded from `model_variables` to inform `MuMIn::dredge`.

```{r load example data, eval = FALSE}
data(model_data)
data(model_variables)
```

Before running `MuMIn::dredge`, model constraints have to be considered to avoid combinations of variables that represented similar landscape characteristics. For example, for each land cover class, only one of each metric type could be present in the fitted model, and similar landscape predictors at different buffer radii were not allowed. To do so, `MuMIn::dredge` accepts expressions for the `subset` argument and such an object is automatically created from `model_variables` below.

```{r subset remote vars, eval = FALSE}

var_comb <- combn(model_variables$variable, 2) %>% # unique combinations of variables
  t() %>% 
  as.data.frame() %>% 
  arrange(V1) %>%
  mutate(subset = paste(V1, "&", V2))
  
# unique combinations of landscape metrics by type
lsm_comb <- landscapemetrics::list_lsm() %>% 
  dplyr::select(metric, type) %>% 
  unique() %>% 
  mutate(metric = paste0("_", metric)) %>% 
  # group_by(type) %>% 
  pivot_wider(names_from = "type", values_from = "metric") %>% 
  apply(MARGIN = 2, FUN = function(x) combn(x[[1]], 2)) %>% 
  as.data.frame() %>% 
  t() %>% 
  as.data.frame()

# get unique landscape classes
lsm_classes <- 
  str_extract(model_variables$predictor, "(?<=lsm_)([^_]*)_") %>% 
  str_remove("_") %>% 
  unique() %>% na.omit()
  
subset_vec <- c()
for(i in 1:nrow(lsm_comb)){ # extract combinations in var_comb that match rows in lsm_comb

  for(j in seq_along(lsm_classes)){

    to_subset <-
      with(var_comb,
           var_comb[grepl(glue::glue("{lsm_classes[j]}{lsm_comb$V1[i]}"), V1) &
                      grepl(glue::glue("{lsm_classes[j]}{lsm_comb$V2[i]}"), V2), ])

    to_subset_inverted <- # if the combination is the other way round
      with(var_comb,
           var_comb[grepl(glue::glue("{lsm_classes[j]}{lsm_comb$V2[i]}"), V1) &
                      grepl(glue::glue("{lsm_classes[j]}{lsm_comb$V1[i]}"), V2), ])

    if(!is.null(to_subset)){
      subset_vec <- c(subset_vec, paste(to_subset$subset))
    }

    if(!is.null(to_subset_inverted)){
      subset_vec <- c(subset_vec, paste(to_subset_inverted$subset))
    }

  }
  rm(to_subset, i, j)
}

subset_exp <- parse(text = paste0("!(", paste(subset_vec, collapse = ") & !("), ")")) # returns exp - subset doesn't accept paste
```

Model data should then be scaled before building the models.

```{r scale model_data, eval = FALSE}
# scale/center model_data

model_data <- model_data %>% 
  mutate(across(.cols = where(is.numeric) & 
                  !sprich, 
                scale))
```

For the `MuMIn::dredge` function, a global model has to be fitted as a primary input. A null model is also fitted for the `extra` argument to calculate a delta-AIC value for model selection.

```{r example dredge - global and null, eval = FALSE}
model_global <- glmer(paste("sprich", "~", paste(model_variables$variable, collapse = " + "), "+ (1|town) "),
                      family = poisson,
                      data = model_data,
                      na.action = "na.fail",
                      control = lme4::glmerControl(optimizer = "bobyqa"))

model_null <- glmer(sprich ~ 1 + (1|town),
                    family = poisson, data = model_data)
```

The `MuMIn::dredge` function is then run with `model_global`, `subset = subset_exp` and `m.lim=c(NA,9)` as model combination constraints. 

```{r example dredge - dredge, eval = FALSE}
model_glm <- dredge(model_global,
                    subset= subset_exp,
                    m.lim=c(NA,9), # max of 9 variables
                    rank="AICc",
                    extra = list(R2 = function(x) r.squaredGLMM(x, model_null)["delta", ]))

# reduce size of huge object - subset only top models 
model_glm <- subset(model_glm, 
                   delta < 8,
                   recalc.weights=TRUE)
```

```{r load dredge model object, include = FALSE}
filepath <- system.file("extdata", "models-remote-post-rf_sample.Rdata", package = "biodivercity")
load(filepath)

data(model_data)

model_data <- model_data %>% 
  mutate(across(.cols = where(is.numeric) & 
                  !sprich, 
                scale)) 
```

The resulting `model_glm` object contains all of the top models with values of ΔAIC < 8. Here, a summary of the top models with values of ΔAIC < 2 can be viewed:

```{r table of best models, echo = FALSE}
bestmodels_info <- subset(model_glm, 
                         delta < 2,
                         recalc.weights=FALSE)

to_print <- bestmodels_info %>% 
  as.data.frame() %>% 
  dplyr::select(where(function(x) any(!is.na(x)))) %>% 
  mutate(across(.cols = !df, ~round(., 3))) %>% 
  mutate(across(.cols = everything(), ~ifelse(is.na(.), "-", .)))

knitr::kable(to_print, caption = glue::glue("**Summary of best models ranked based on automated model selection from `MuMIn::dredge()` (ΔAIC < 2).**")) %>%
  kable_styling("striped") %>% scroll_box(width = "100%", height = "300px")
```

Furthermore, to View the breakdown of variables in the top models, a plot of the top important variables and their sum of model weights across all models can be produced:

```{r plot of coefs, message = FALSE}
coef <- data.frame(coef(bestmodels_info)[,-1]) %>% 
  summarise(across(everything(), ~ mean(.x, na.rm = TRUE))) %>% 
  pivot_longer(everything(), names_to = "var") %>% 
  mutate(effect = ifelse(value > 0, "Positive", "Negative"))

imp <- data.frame(MuMIn::sw(bestmodels_info)) %>% 
  rename(imp = MuMIn..sw.bestmodels_info.) %>% 
  rownames_to_column("var") %>% 
  left_join(coef, by = "var") %>% 
  mutate(effect = ifelse(is.na(effect), "Mixed (factor)", effect)) %>%
  mutate(effect = factor(effect, levels = c("Positive", "Negative", "Mixed (factor)")))

ggplot(data=imp, aes(x = imp, y = reorder(var, imp), 
                     fill = effect)) + 
  geom_bar(stat = "identity") +
  labs(x = "Sum of weights", y = "Variables") +
  scale_fill_manual(values = c("#4daf4a", "#e41a1c", "#377eb8"),
                    name = "Effect")

knitr::kable(coef, caption = glue::glue("**Mean coefficient value for each predictor within the best models.**")) %>%
  kable_styling("striped") %>% scroll_box(width = "100%", height = "300px")

```

```{r, include = FALSE}
rm(filepath, to_print, coef, imp)

# bestmodels <- MuMIn::get.models(bestmodels_info, subset = TRUE)
# performance::check_model(bestmodels[[1]])

rm(bestmodels_info)
```

<br>

---

## References

Arthur AD, Li J, Henry S & Cunningham SA (2010) Influence of woody vegetation on pollinator densities in oilseed Brassica fields in an Australian temperate landscape. _Basic and Applied Ecology_, _11_(5): 406–414.

Hothorn T, Hornik K & Zeileis A (2006) Unbiased recursive partitioning: A conditional inference framework. _Journal of Computational and Graphical Statistics_, _15_(3): 651–674.

Li J, Alvarez B, Siwabessy J, Tran M, Huang Z, Przeslawski R, Radke L, Howard F & Nichol S (2017) Application of random forest and generalised linear model and their hybrid methods with geostatistical techniques to count data: Predicting sponge species richness. _Environmental Modelling and Software_, _97_: 112–129.
